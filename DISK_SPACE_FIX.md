# ğŸ’¾ Disk Space Issue & Fix

## âŒ é—®é¢˜ï¼šè®­ç»ƒæ—¶ç£ç›˜ç©ºé—´ä¸è¶³

```
OSError: [Errno 28] No space left on device
```

---

## ğŸ” æ ¹æœ¬åŸå› 

### **SE-ST-Combined vs STATE çš„ Checkpoint å¤§å°å¯¹æ¯”**

| æ¨¡å‹ | Trainable | Frozen | **Checkpoint å¤§å°** |
|------|-----------|--------|-------------------|
| **STATE** | 50M params | 0 | **~200 MB** |
| **SE-ST-Combinedï¼ˆä½ çš„ï¼‰** | 48.9M params | **600M (SE)** | **~2.5 GB** |

### **é—®é¢˜è¯¦è§£**

1. **Lightning é»˜è®¤è¡Œä¸º**ï¼š
   - æ¯æ¬¡éªŒè¯ï¼ˆ`val_check_interval`ï¼‰ä¿å­˜ `last.ckpt`
   - ä¿å­˜ top-k æœ€å¥½çš„ checkpoints
   - **ä¿å­˜æ•´ä¸ªæ¨¡å‹çŠ¶æ€**ï¼ˆåŒ…æ‹¬å†»ç»“çš„å‚æ•°ï¼ï¼‰

2. **ä½ çš„è®­ç»ƒè®¾ç½®**ï¼š
   - `val_check_interval=250` steps
   - `max_steps=40000`
   - 40000 / 250 = **160 æ¬¡éªŒè¯**
   - 160 Ã— 2.5 GB = **400 GB ç£ç›˜ç©ºé—´ï¼**

3. **STATE ä¸ºä»€ä¹ˆæ²¡é—®é¢˜ï¼Ÿ**
   - STATE æ²¡æœ‰å†»ç»“çš„å¤§æ¨¡å‹
   - Checkpoint åªæœ‰ 200 MB
   - 160 Ã— 200 MB = 32 GBï¼ˆå¯æ¥å—ï¼‰

---

## âœ… è§£å†³æ–¹æ¡ˆ

### **ä¿®å¤ 1: ä¸ä¿å­˜ `last.ckpt`ï¼ˆå·²åº”ç”¨ï¼‰**

ä¿®æ”¹äº† `se_st_combined/cli/train.py`ï¼š

```python
checkpoint_callback = ModelCheckpoint(
    save_last=False,  # â† ä¸åœ¨æ¯æ¬¡éªŒè¯æ—¶ä¿å­˜ last.ckpt
    save_on_train_epoch_end=False,  # åªåœ¨éªŒè¯æ—¶ä¿å­˜
    save_top_k=3,  # åªä¿å­˜æœ€å¥½çš„ 3 ä¸ª
)
```

**æ•ˆæœ**ï¼š
- ç£ç›˜ä½¿ç”¨ï¼š400 GB â†’ **~7.5 GB**ï¼ˆtop-3 onlyï¼‰
- ä»ç„¶æ¯ `ckpt_every_n_steps` (5000) ä¿å­˜ä¸€æ¬¡ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰

---

### **ä¿®å¤ 2: æ¸…ç†ç£ç›˜ç©ºé—´**

```bash
# 1. æ¸…ç†æ—§çš„ checkpoints
rm -rf competition/*/checkpoints/*.ckpt
rm -rf competition/*/last.ckpt

# 2. æ¸…ç† pip ç¼“å­˜
pip cache purge

# 3. æ¸…ç† PyTorch ç¼“å­˜
rm -rf ~/.cache/torch

# 4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf /tmp/*

# 5. æ£€æŸ¥ç£ç›˜ä½¿ç”¨
df -h
```

---

### **ä¿®å¤ 3: è°ƒæ•´è®­ç»ƒå‚æ•°ï¼ˆå¯é€‰ï¼‰**

å¦‚æœç£ç›˜ç©ºé—´ä»ç„¶ç´§å¼ ï¼Œå¯ä»¥ï¼š

#### **A. å‡å°‘ä¿å­˜çš„ checkpoint æ•°é‡**
```bash
++save_top_k=1  # åªä¿å­˜æœ€å¥½çš„ 1 ä¸ªï¼ˆè€Œé 3 ä¸ªï¼‰
```

#### **B. å¢åŠ ä¿å­˜é—´éš”**
```bash
++training.ckpt_every_n_steps=10000  # æ¯ 10k æ­¥ä¿å­˜ï¼ˆè€Œé 5kï¼‰
```

#### **C. ä½¿ç”¨æ›´å¤§çš„éªŒè¯é—´éš”**
```bash
++training.val_check_interval=490  # æ¯ä¸ª epoch éªŒè¯ä¸€æ¬¡
```

---

## ğŸ“Š å®Œæ•´è®­ç»ƒå‘½ä»¤ï¼ˆå¸¦ä¼˜åŒ–ï¼‰

```bash
# å…ˆæ¸…ç†ç£ç›˜
rm -rf competition/*/checkpoints/*.ckpt
pip cache purge

# æ›´æ–°ä»£ç 
cd /workspace/se-st-combined
git pull origin main
pip install -e .

# é‡æ–°è®­ç»ƒï¼ˆä½¿ç”¨ä¿®å¤åçš„é…ç½®ï¼‰
se-st-train \
  ++data.kwargs.toml_config_path="data/starter.toml" \
  ++data.kwargs.perturbation_features_file="/data/ESM2_pert_features.pt" \
  ++data.kwargs.num_workers=8 \
  ++training.max_steps=40000 \
  ++training.ckpt_every_n_steps=5000 \
  ++training.batch_size=16 \
  ++training.lr=1e-4 \
  ++training.val_check_interval=250 \
  ++training.log_every_n_steps=50 \
  ++save_top_k=2 \
  ++model.kwargs.input_dim=18080 \
  ++model.kwargs.hidden_dim=512 \
  ++model.kwargs.output_dim=18080 \
  ++model.kwargs.pert_dim=5120 \
  ++model.kwargs.se_model_path="SE-600M" \
  ++model.kwargs.se_checkpoint_path="SE-600M/se600m_epoch15.ckpt" \
  ++output_dir="competition" \
  ++name="se_st_combined_40k_fixed"
```

**é¢„æœŸç£ç›˜ä½¿ç”¨**ï¼š
- Top-2 checkpoints: 5 GB
- 8 periodic checkpoints (æ¯ 5k æ­¥): 20 GB
- **æ€»è®¡**: ~25 GBï¼ˆå¯æ§ï¼‰

---

## ğŸ”® æœªæ¥ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰

### **Option 1: åªä¿å­˜ trainable parameters**

ç†è®ºä¸Šå¯ä»¥åœ¨ä¿å­˜æ—¶æ’é™¤å†»ç»“çš„ SE encoderï¼Œä½†éœ€è¦è‡ªå®šä¹‰ `on_save_checkpoint` é’©å­ï¼š

```python
def on_save_checkpoint(self, checkpoint):
    # ç§»é™¤å†»ç»“çš„ SE encoder å‚æ•°
    state_dict = checkpoint['state_dict']
    filtered = {k: v for k, v in state_dict.items() 
                if not k.startswith('se_model.')}
    checkpoint['state_dict'] = filtered
    return checkpoint
```

**é—®é¢˜**ï¼šåŠ è½½æ—¶éœ€è¦é‡æ–°åŠ è½½ SE encoderï¼Œå¢åŠ å¤æ‚åº¦ã€‚

---

### **Option 2: ä½¿ç”¨å¤–éƒ¨å­˜å‚¨**

å¦‚æœæœ‰äº‘å­˜å‚¨ï¼ˆS3/GCSï¼‰ï¼Œå¯ä»¥é…ç½® Lightning ç›´æ¥ä¿å­˜åˆ°äº‘ç«¯ï¼š

```python
checkpoint_callback = ModelCheckpoint(
    dirpath="s3://your-bucket/checkpoints",
    ...
)
```

---

## ğŸ“ ç›‘æ§ç£ç›˜ä½¿ç”¨

è®­ç»ƒæ—¶å®šæœŸæ£€æŸ¥ï¼š

```bash
# æ¯éš” 10 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
watch -n 600 df -h

# æˆ–è€…åœ¨åå°ç›‘æ§
while true; do 
    df -h | grep '/dev/sda1'
    sleep 600
done
```

---

## â“ FAQ

### Q1: ä¸ºä»€ä¹ˆä¸ç”¨ `save_weights_only=True`ï¼Ÿ
**A**: å³ä½¿ä½¿ç”¨ `save_weights_only=True`ï¼ŒLightning ä»ä¼šä¿å­˜æ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬å†»ç»“çš„ï¼‰ã€‚ä½ éœ€è¦è‡ªå®šä¹‰ä¿å­˜é€»è¾‘æ‰èƒ½çœŸæ­£æ’é™¤å†»ç»“å‚æ•°ã€‚

### Q2: å¦‚æœè®­ç»ƒä¸­æ–­äº†æ€ä¹ˆåŠï¼Ÿ
**A**: æ¯ 5000 steps ä»ä¼šä¿å­˜ä¸€ä¸ª checkpointï¼Œå¯ä»¥ä»æœ€è¿‘çš„æ¢å¤ï¼š
```bash
++model.checkpoint="competition/se_st_combined_40k/checkpoints/step=35000-*.ckpt"
```

### Q3: STATE æ˜¯æ€ä¹ˆå¤„ç†çš„ï¼Ÿ
**A**: STATE æ²¡æœ‰å†»ç»“çš„å¤§æ¨¡å‹ï¼Œæ‰€ä»¥æ¯ä¸ª checkpoint åªæœ‰ 200 MBï¼Œä¸ä¼šé‡åˆ°è¿™ä¸ªé—®é¢˜ã€‚

### Q4: èƒ½ä¸èƒ½å®Œå…¨ä¸ä¿å­˜ checkpointï¼Ÿ
**A**: ä¸æ¨èï¼å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œä½ ä¼šå¤±å»æ‰€æœ‰è¿›åº¦ã€‚è‡³å°‘ä¿å­˜ top-1 æˆ– periodic checkpointsã€‚

---

## ğŸ¯ æ€»ç»“

| ç­–ç•¥ | ç£ç›˜èŠ‚çœ | é£é™© |
|------|---------|------|
| **ä¸ä¿å­˜ `last.ckpt`** | 95% (400GB â†’ 20GB) | âœ… ä½ï¼ˆä»æœ‰ top-k å’Œ periodicï¼‰ |
| **`save_top_k=1`** | é¢å¤– 2.5 GB | âš ï¸ ä¸­ï¼ˆåªæœ‰ 1 ä¸ªæœ€å¥½çš„ï¼‰ |
| **å¢åŠ  `ckpt_every_n_steps`** | é¢å¤– 10+ GB | âš ï¸ ä¸­ï¼ˆæ¢å¤ç‚¹æ›´å°‘ï¼‰ |
| **è‡ªå®šä¹‰ä¿å­˜é€»è¾‘** | 90% (2.5GB â†’ 200MB) | âš ï¸ é«˜ï¼ˆåŠ è½½å¤æ‚ï¼‰ |

**æ¨è**ï¼šä½¿ç”¨å½“å‰ä¿®å¤ï¼ˆä¸ä¿å­˜ `last.ckpt`ï¼‰+ `save_top_k=2-3` + `ckpt_every_n_steps=5000`ã€‚

è¿™æ ·æ—¢èƒ½èŠ‚çœç£ç›˜ç©ºé—´ï¼Œåˆèƒ½ä¿è¯è®­ç»ƒçš„é²æ£’æ€§ã€‚

